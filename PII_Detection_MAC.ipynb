{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from presidio_analyzer import AnalyzerEngine, PatternRecognizer\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "import glob\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "import textract\n",
    "from pathlib import Path\n",
    "import stat\n",
    "import time\n",
    "import datetime\n",
    "import easyocr\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "from csv import reader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "from abc import abstractmethod\n",
    "from typing import List, Dict\n",
    "\n",
    "from presidio_analyzer import RecognizerResult\n",
    "from presidio_analyzer.nlp_engine import NlpArtifacts\n",
    "\n",
    "logger = logging.getLogger(\"presidio-analyzer\")\n",
    "\n",
    "\n",
    "class EntityRecognizer:\n",
    "    \"\"\"\n",
    "    A class representing an abstract PII entity recognizer.\n",
    "    EntityRecognizer is an abstract class to be inherited by\n",
    "    Recognizers which hold the logic for recognizing specific PII entities.\n",
    "    :param supported_entities: the entities supported by this recognizer\n",
    "    (for example, phone number, address, etc.)\n",
    "    :param supported_language: the language supported by this recognizer.\n",
    "    The supported langauge code is iso6391Name\n",
    "    :param name: the name of this recognizer (optional)\n",
    "    :param version: the recognizer current version\n",
    "    \"\"\"\n",
    "\n",
    "    MIN_SCORE = 0\n",
    "    MAX_SCORE = 1.0\n",
    "    CONTEXT_SIMILARITY_THRESHOLD = 0.65\n",
    "    CONTEXT_SIMILARITY_FACTOR = 0.35\n",
    "    MIN_SCORE_WITH_CONTEXT_SIMILARITY = 0.4\n",
    "    CONTEXT_PREFIX_COUNT = 5\n",
    "    CONTEXT_SUFFIX_COUNT = 0\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        supported_entities: List[str],\n",
    "        name: str = None,\n",
    "        supported_language: str = \"en\",\n",
    "        version: str = \"0.0.1\",\n",
    "    ):\n",
    "\n",
    "        self.supported_entities = supported_entities\n",
    "\n",
    "        if name is None:\n",
    "            self.name = self.__class__.__name__  # assign class name as name\n",
    "        else:\n",
    "            self.name = name\n",
    "\n",
    "        self.supported_language = supported_language\n",
    "        self.version = version\n",
    "        self.is_loaded = False\n",
    "\n",
    "        self.load()\n",
    "        logger.info(\"Loaded recognizer: %s\", self.name)\n",
    "        self.is_loaded = True\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Initialize the recognizer assets if needed.\n",
    "        (e.g. machine learning models)\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def analyze(\n",
    "        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n",
    "    ):# -> List[RecognizerResult]\n",
    "        \"\"\"\n",
    "        Analyze text to identify entities.\n",
    "        :param text: The text to be analyzed\n",
    "        :param entities: The list of entities this recognizer is able to detect\n",
    "        :param nlp_artifacts: A group of attributes which are the result of\n",
    "        an NLP process over the input text.\n",
    "        :return: List of results detected by this recognizer.\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    def get_supported_entities(self): # -> List[str]\n",
    "        \"\"\"\n",
    "        Return the list of entities this recognizer can identify.\n",
    "        :return: A list of the supported entities by this recognizer\n",
    "        \"\"\"\n",
    "        return self.supported_entities\n",
    "\n",
    "    def get_supported_language(self): # -> str\n",
    "        \"\"\"\n",
    "        Return the language this recognizer can support.\n",
    "        :return: A list of the supported language by this recognizer\n",
    "        \"\"\"\n",
    "        return self.supported_language\n",
    "\n",
    "    def get_version(self): # -> str\n",
    "        \"\"\"\n",
    "        Return the version of this recognizer.\n",
    "        :return: The current version of this recognizer\n",
    "        \"\"\"\n",
    "        return self.version\n",
    "\n",
    "    def to_dict(self): # -> Dict\n",
    "        \"\"\"\n",
    "        Serialize self to dictionary.\n",
    "        :return: a dictionary\n",
    "        \"\"\"\n",
    "        return_dict = {\n",
    "            \"supported_entities\": self.supported_entities,\n",
    "            \"supported_language\": self.supported_language,\n",
    "            \"name\": self.name,\n",
    "            \"version\": self.version,\n",
    "        }\n",
    "        return return_dict\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, entity_recognizer_dict: Dict): # -> \"EntityRecognizer\"\n",
    "        \"\"\"\n",
    "        Create EntityRecognizer from a dict input.\n",
    "        :param entity_recognizer_dict: Dict containing keys and values for instantiation\n",
    "        \"\"\"\n",
    "        return cls(**entity_recognizer_dict)\n",
    "\n",
    "    def enhance_using_context(\n",
    "        self,\n",
    "        text: str,\n",
    "        raw_results: List[RecognizerResult],\n",
    "        nlp_artifacts: NlpArtifacts,\n",
    "        recognizer_context_words: List[str],\n",
    "    ): # -> List[RecognizerResult]\n",
    "        \"\"\"\n",
    "        Update results in case surrounding words are relevant to the context words.\n",
    "        Using the surrounding words of the actual word matches, look\n",
    "        for specific strings that if found contribute to the score\n",
    "        of the result, improving the confidence that the match is\n",
    "        indeed of that PII entity type\n",
    "        :param text: The actual text that was analyzed\n",
    "        :param raw_results: Recognizer results which didn't take\n",
    "                            context into consideration\n",
    "        :param nlp_artifacts: The nlp artifacts contains elements\n",
    "                              such as lemmatized tokens for better\n",
    "                              accuracy of the context enhancement process\n",
    "        :param recognizer_context_words: The words the current recognizer\n",
    "                                         supports (words to lookup)\n",
    "        \"\"\"\n",
    "        # create a deep copy of the results object so we can manipulate it\n",
    "        results = copy.deepcopy(raw_results)\n",
    "\n",
    "        # Sanity\n",
    "        if nlp_artifacts is None:\n",
    "            logger.warning(\"[%s]. NLP artifacts were not provided\", self.name)\n",
    "            return results\n",
    "        if recognizer_context_words is None or recognizer_context_words == []:\n",
    "            logger.info(\n",
    "                \"recognizer '%s' does not support context \" \"enhancement\", self.name\n",
    "            )\n",
    "            return results\n",
    "\n",
    "        for result in results:\n",
    "            # extract lemmatized context from the surrounding of the match\n",
    "\n",
    "            word = text[result.start : result.end]\n",
    "\n",
    "            surrounding_words = self.__extract_surrounding_words(\n",
    "                nlp_artifacts=nlp_artifacts, word=word, start=result.start\n",
    "            )\n",
    "\n",
    "            supportive_context_word = self.__find_supportive_word_in_context(\n",
    "                surrounding_words, recognizer_context_words\n",
    "            )\n",
    "            if supportive_context_word != \"\":\n",
    "                result.score += self.CONTEXT_SIMILARITY_FACTOR\n",
    "                result.score = max(result.score, self.MIN_SCORE_WITH_CONTEXT_SIMILARITY)\n",
    "                result.score = min(result.score, EntityRecognizer.MAX_SCORE)\n",
    "\n",
    "                # Update the explainability object with context information\n",
    "                # helped improving the score\n",
    "                result.analysis_explanation.set_supportive_context_word(\n",
    "                    supportive_context_word\n",
    "                )\n",
    "                result.analysis_explanation.set_improved_score(result.score)\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def __context_to_keywords(context: str): # -> List[str]\n",
    "        return context.split(\" \")\n",
    "\n",
    "    @staticmethod\n",
    "    def __find_supportive_word_in_context(\n",
    "        context_list: List[str], recognizer_context_list: List[str]\n",
    "    ): # -> str\n",
    "        \"\"\"\n",
    "        Find words in the text which are relevant for context evaluation.\n",
    "        A word is considered a supportive context word if there's exact match\n",
    "        between a keyword in context_text and any keyword in context_list.\n",
    "        :param context_list words before and after the matched entity within\n",
    "               a specified window size\n",
    "        :param recognizer_context_list a list of words considered as\n",
    "                context keywords manually specified by the recognizer's author\n",
    "        \"\"\"\n",
    "        word = \"\"\n",
    "        # If the context list is empty, no need to continue\n",
    "        if context_list is None or recognizer_context_list is None:\n",
    "            return word\n",
    "\n",
    "        for predefined_context_word in recognizer_context_list:\n",
    "            # result == true only if any of the predefined context words\n",
    "            # is found exactly or as a substring in any of the collected\n",
    "            # context words\n",
    "            result = next(\n",
    "                (\n",
    "                    True\n",
    "                    for keyword in context_list\n",
    "                    if predefined_context_word in keyword\n",
    "                ),\n",
    "                False,\n",
    "            )\n",
    "            if result:\n",
    "                logger.debug(\"Found context keyword '%s'\", predefined_context_word)\n",
    "                word = predefined_context_word\n",
    "                break\n",
    "\n",
    "        return word\n",
    "\n",
    "    @staticmethod\n",
    "    def __add_n_words(\n",
    "        index: int,\n",
    "        n_words: int,\n",
    "        lemmas: List[str],\n",
    "        lemmatized_filtered_keywords: List[str],\n",
    "        is_backward: bool,\n",
    "    ): # -> List[str]\n",
    "        \"\"\"\n",
    "        Prepare a string of context words.\n",
    "        Return a list of words which surrounds a lemma at a given index.\n",
    "        The words will be collected only if exist in the filtered array\n",
    "        :param index: index of the lemma that its surrounding words we want\n",
    "        :param n_words: number of words to take\n",
    "        :param lemmas: array of lemmas\n",
    "        :param lemmatized_filtered_keywords: the array of filtered\n",
    "               lemmas from the original sentence,\n",
    "        :param is_backward: if true take the preceeding words, if false,\n",
    "                            take the successing words\n",
    "        \"\"\"\n",
    "        i = index\n",
    "        context_words = []\n",
    "        # The entity itself is no interest to us...however we want to\n",
    "        # consider it anyway for cases were it is attached with no spaces\n",
    "        # to an interesting context word, so we allow it and add 1 to\n",
    "        # the number of collected words\n",
    "\n",
    "        # collect at most n words (in lower case)\n",
    "        remaining = n_words + 1\n",
    "        while 0 <= i < len(lemmas) and remaining > 0:\n",
    "            lower_lemma = lemmas[i].lower()\n",
    "            if lower_lemma in lemmatized_filtered_keywords:\n",
    "                context_words.append(lower_lemma)\n",
    "                remaining -= 1\n",
    "            i = i - 1 if is_backward else i + 1\n",
    "        return context_words\n",
    "\n",
    "    def __add_n_words_forward(\n",
    "        self,\n",
    "        index: int,\n",
    "        n_words: int,\n",
    "        lemmas: List[str],\n",
    "        lemmatized_filtered_keywords: List[str],\n",
    "    ): # -> List[str]\n",
    "        return self.__add_n_words(\n",
    "            index, n_words, lemmas, lemmatized_filtered_keywords, False\n",
    "        )\n",
    "\n",
    "    def __add_n_words_backward(\n",
    "        self,\n",
    "        index: int,\n",
    "        n_words: int,\n",
    "        lemmas: List[str],\n",
    "        lemmatized_filtered_keywords: List[str],\n",
    "    ): # -> List[str]\n",
    "        return self.__add_n_words(\n",
    "            index, n_words, lemmas, lemmatized_filtered_keywords, True\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _find_index_of_match_token(\n",
    "        word: str, start: int, tokens, tokens_indices: List[int]  # noqa ANN001\n",
    "    ): # -> int\n",
    "        found = False\n",
    "        # we use the known start index of the original word to find the actual\n",
    "        # token at that index, we are not checking for equivilance since the\n",
    "        # token might be just a substring of that word (e.g. for phone number\n",
    "        # 555-124564 the first token might be just '555' or for a match like '\n",
    "        # rocket' the actual token will just be 'rocket' hence the misalignment\n",
    "        # of indices)\n",
    "        # Note: we are iterating over the original tokens (not the lemmatized)\n",
    "        i = -1\n",
    "        for i, token in enumerate(tokens, 0):\n",
    "            # Either we found a token with the exact location, or\n",
    "            # we take a token which its characters indices covers\n",
    "            # the index we are looking for.\n",
    "            if (tokens_indices[i] == start) or (start < tokens_indices[i] + len(token)):\n",
    "                # found the interesting token, the one that around it\n",
    "                # we take n words, we save the matching lemma\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "        if not found:\n",
    "            raise ValueError(\n",
    "                \"Did not find word '\" + word + \"' \"\n",
    "                \"in the list of tokens although it \"\n",
    "                \"is expected to be found\"\n",
    "            )\n",
    "        return i\n",
    "\n",
    "    def __extract_surrounding_words(\n",
    "        self, nlp_artifacts: NlpArtifacts, word: str, start: int\n",
    "    ): # -> List[str]\n",
    "        \"\"\"Extract words surrounding another given word.\n",
    "        The text from which the context is extracted is given in the nlp\n",
    "        doc.\n",
    "        :param nlp_artifacts: An abstraction layer which holds different\n",
    "                              items which are the result of a NLP pipeline\n",
    "                              execution on a given text\n",
    "        :param word: The word to look for context around\n",
    "        :param start: The start index of the word in the original text\n",
    "        \"\"\"\n",
    "        if not nlp_artifacts.tokens:\n",
    "            logger.info(\"Skipping context extraction due to \" \"lack of NLP artifacts\")\n",
    "            # if there are no nlp artifacts, this is ok, we can\n",
    "            # extract context and we return a valid, yet empty\n",
    "            # context\n",
    "            return [\"\"]\n",
    "\n",
    "        # Get the already prepared words in the given text, in their\n",
    "        # LEMMATIZED version\n",
    "        lemmatized_keywords = nlp_artifacts.keywords\n",
    "\n",
    "        # since the list of tokens is not necessarily aligned\n",
    "        # with the actual index of the match, we look for the\n",
    "        # token index which corresponds to the match\n",
    "        token_index = EntityRecognizer._find_index_of_match_token(\n",
    "            word, start, nlp_artifacts.tokens, nlp_artifacts.tokens_indices\n",
    "        )\n",
    "\n",
    "        # index i belongs to the PII entity, take the preceding n words\n",
    "        # and the successing m words into a context list\n",
    "\n",
    "        backward_context = self.__add_n_words_backward(\n",
    "            token_index,\n",
    "            EntityRecognizer.CONTEXT_PREFIX_COUNT,\n",
    "            nlp_artifacts.lemmas,\n",
    "            lemmatized_keywords,\n",
    "        )\n",
    "        forward_context = self.__add_n_words_forward(\n",
    "            token_index,\n",
    "            EntityRecognizer.CONTEXT_SUFFIX_COUNT,\n",
    "            nlp_artifacts.lemmas,\n",
    "            lemmatized_keywords,\n",
    "        )\n",
    "\n",
    "        context_list = []\n",
    "        context_list.extend(backward_context)\n",
    "        context_list.extend(forward_context)\n",
    "        context_list = list(set(context_list))\n",
    "        logger.debug(\"Context list is: %s\", \" \".join(context_list))\n",
    "        return context_list\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_duplicates(results: List[RecognizerResult]): # -> List[RecognizerResult]\n",
    "        \"\"\"\n",
    "        Remove duplicate results.\n",
    "        Remove duplicates in case the two results\n",
    "        have identical start and ends and types.\n",
    "        :param results: List[RecognizerResult]\n",
    "        :return: List[RecognizerResult]\n",
    "        \"\"\"\n",
    "        results = list(set(results))\n",
    "        results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n",
    "        filtered_results = []\n",
    "\n",
    "        for result in results:\n",
    "            if result.score == 0:\n",
    "                continue\n",
    "\n",
    "            to_keep = result not in filtered_results  # equals based comparison\n",
    "            if to_keep:\n",
    "                for filtered in filtered_results:\n",
    "                    # If result is contained in one of the other results\n",
    "                    if (\n",
    "                        result.contained_in(filtered)\n",
    "                        and result.entity_type == filtered.entity_type\n",
    "                    ):\n",
    "                        to_keep = False\n",
    "                        break\n",
    "\n",
    "            if to_keep:\n",
    "                filtered_results.append(result)\n",
    "\n",
    "        return filtered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "from typing import Optional, List, Iterable, Union, Type\n",
    "\n",
    "from presidio_analyzer import EntityRecognizer\n",
    "from presidio_analyzer.nlp_engine import NlpEngine, SpacyNlpEngine, StanzaNlpEngine\n",
    "from presidio_analyzer.predefined_recognizers import (\n",
    "    CreditCardRecognizer,\n",
    "    CryptoRecognizer,\n",
    "    DateRecognizer,\n",
    "    DomainRecognizer,\n",
    "    EmailRecognizer,\n",
    "    IbanRecognizer,\n",
    "    IpRecognizer,\n",
    "    MedicalLicenseRecognizer,\n",
    "    NhsRecognizer,\n",
    "    PhoneRecognizer,\n",
    "    UsBankRecognizer,\n",
    "    UsLicenseRecognizer,\n",
    "    UsItinRecognizer,\n",
    "    UsPassportRecognizer,\n",
    "    UsSsnRecognizer,\n",
    "    SgFinRecognizer,\n",
    "    SpacyRecognizer,\n",
    "    EsNifRecognizer,\n",
    "    StanzaRecognizer,\n",
    "    AuAbnRecognizer,\n",
    "    AuAcnRecognizer,\n",
    "    AuTfnRecognizer,\n",
    "    AuMedicareRecognizer,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"presidio-analyzer\")\n",
    "\n",
    "\n",
    "class RecognizerRegistry:\n",
    "    \"\"\"\n",
    "    Detect, register and hold all recognizers to be used by the analyzer.\n",
    "\n",
    "    :param recognizers: An optional list of recognizers,\n",
    "    that will be available instead of the predefined recognizers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, recognizers: Optional[Iterable[EntityRecognizer]] = None):\n",
    "\n",
    "        if recognizers:\n",
    "            self.recognizers = recognizers\n",
    "        else:\n",
    "            self.recognizers = []\n",
    "\n",
    "    def load_predefined_recognizers(\n",
    "        self, languages: Optional[List[str]] = None, nlp_engine: NlpEngine = None\n",
    "    ): # -> None\n",
    "        \"\"\"\n",
    "        Load the existing recognizers into memory.\n",
    "\n",
    "        :param languages: List of languages for which to load recognizers\n",
    "        :param nlp_engine: The NLP engine to use.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if not languages:\n",
    "            languages = [\"en\"]\n",
    "\n",
    "        nlp_recognizer = self._get_nlp_recognizer(nlp_engine)\n",
    "        recognizers_map = {\n",
    "            \"en\": [\n",
    "                UsBankRecognizer,\n",
    "                UsLicenseRecognizer,\n",
    "                UsItinRecognizer,\n",
    "                UsPassportRecognizer,\n",
    "                UsSsnRecognizer,\n",
    "                NhsRecognizer,\n",
    "                SgFinRecognizer,\n",
    "                AuAbnRecognizer,\n",
    "                AuAcnRecognizer,\n",
    "                AuTfnRecognizer,\n",
    "                AuMedicareRecognizer,\n",
    "            ],\n",
    "            \"es\": [EsNifRecognizer],\n",
    "            \"ALL\": [\n",
    "                CreditCardRecognizer,\n",
    "                CryptoRecognizer,\n",
    "                DateRecognizer,\n",
    "                DomainRecognizer,\n",
    "                EmailRecognizer,\n",
    "                IbanRecognizer,\n",
    "                IpRecognizer,\n",
    "                MedicalLicenseRecognizer,\n",
    "                nlp_recognizer,\n",
    "                PhoneRecognizer,\n",
    "            ],\n",
    "        }\n",
    "        for lang in languages:\n",
    "            lang_recognizers = [rc() for rc in recognizers_map.get(lang, [])]\n",
    "            self.recognizers.extend(lang_recognizers)\n",
    "            all_recognizers = [\n",
    "                rc(supported_language=lang) for rc in recognizers_map.get(\"ALL\", [])\n",
    "            ]\n",
    "            self.recognizers.extend(all_recognizers)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_nlp_recognizer(\n",
    "        nlp_engine: NlpEngine,\n",
    "    ): # -> Union[Type[SpacyRecognizer], Type[StanzaRecognizer]]\n",
    "        \"\"\"Return the recognizer leveraging the selected NLP Engine.\"\"\"\n",
    "\n",
    "        if not nlp_engine or type(nlp_engine) == SpacyNlpEngine:\n",
    "            return SpacyRecognizer\n",
    "        if isinstance(nlp_engine, StanzaNlpEngine):\n",
    "            return StanzaRecognizer\n",
    "        else:\n",
    "            logger.warning(\n",
    "                \"nlp engine should be either SpacyNlpEngine or StanzaNlpEngine\"\n",
    "            )\n",
    "            # Returning default\n",
    "            return SpacyRecognizer\n",
    "\n",
    "    def get_recognizers(\n",
    "        self,\n",
    "        language: str,\n",
    "        entities: Optional[List[str]] = None,\n",
    "        all_fields: bool = False,\n",
    "        ad_hoc_recognizers: Optional[List[EntityRecognizer]] = None,\n",
    "    ):  #-> List[EntityRecognizer]\n",
    "        \"\"\"\n",
    "        Return a list of recognizers which supports the specified name and language.\n",
    "\n",
    "        :param entities: the requested entities\n",
    "        :param language: the requested language\n",
    "        :param all_fields: a flag to return all fields of a requested language.\n",
    "        :param ad_hoc_recognizers: Additional recognizers provided by the user\n",
    "        as part of the request\n",
    "        :return: A list of the recognizers which supports the supplied entities\n",
    "        and language\n",
    "        \"\"\"\n",
    "        if language is None:\n",
    "            raise ValueError(\"No language provided\")\n",
    "\n",
    "        if entities is None and all_fields is False:\n",
    "            raise ValueError(\"No entities provided\")\n",
    "\n",
    "        all_possible_recognizers = copy.copy(self.recognizers)\n",
    "        if ad_hoc_recognizers:\n",
    "            all_possible_recognizers.extend(ad_hoc_recognizers)\n",
    "\n",
    "        # filter out unwanted recognizers\n",
    "        to_return = set()\n",
    "        if all_fields:\n",
    "            to_return = [\n",
    "                rec\n",
    "                for rec in all_possible_recognizers\n",
    "                if language == rec.supported_language\n",
    "            ]\n",
    "        else:\n",
    "            for entity in entities:\n",
    "                subset = [\n",
    "                    rec\n",
    "                    for rec in all_possible_recognizers\n",
    "                    if entity in rec.supported_entities\n",
    "                    and language == rec.supported_language\n",
    "                ]\n",
    "\n",
    "                if not subset:\n",
    "                    logger.warning(\n",
    "                        \"Entity %s doesn't have the corresponding\"\n",
    "                        \" recognizer in language : %s\",\n",
    "                        entity,\n",
    "                        language,\n",
    "                    )\n",
    "                else:\n",
    "                    to_return.update(set(subset))\n",
    "\n",
    "        logger.debug(\n",
    "            \"Returning a total of %s recognizers\",\n",
    "            str(len(to_return)),\n",
    "        )\n",
    "\n",
    "        if not to_return:\n",
    "            raise ValueError(\"No matching recognizers were found to serve the request.\")\n",
    "\n",
    "        return list(to_return)\n",
    "\n",
    "    def add_recognizer(self, recognizer: EntityRecognizer): # -> None\n",
    "        \"\"\"\n",
    "        Add a new recognizer to the list of recognizers.\n",
    "\n",
    "        :param recognizer: Recognizer to add\n",
    "        \"\"\"\n",
    "        if not isinstance(recognizer, EntityRecognizer):\n",
    "            raise ValueError(\"Input is not of type EntityRecognizer\")\n",
    "\n",
    "        self.recognizers.append(recognizer)\n",
    "\n",
    "    def remove_recognizer(self, recognizer_name: str): # -> None\n",
    "        \"\"\"\n",
    "        Remove a recognizer based on its name.\n",
    "\n",
    "        :param recognizer_name: Name of recognizer to remove\n",
    "        \"\"\"\n",
    "        new_recognizers = [\n",
    "            rec for rec in self.recognizers if rec.name != recognizer_name\n",
    "        ]\n",
    "        logger.info(\n",
    "            \"Removed %s recognizers which had the name %s\",\n",
    "            str(len(self.recognizers) - len(new_recognizers)),\n",
    "            recognizer_name,\n",
    "        )\n",
    "        self.recognizers = new_recognizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def ocr_pii(img):\n",
    "   \n",
    "    img_gray = ImageOps.grayscale(img)\n",
    "    # increase image contrast\n",
    "    img_gray_contrast_incr = ImageEnhance.Contrast(img_gray).enhance(1)\n",
    "    languages = ['en']\n",
    "    reader = easyocr.Reader(languages)\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    img_gray_contrast_incr.save(img_byte_arr, format='png')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "    return reader.readtext(img_byte_arr, detail= 0, paragraph=True)\n",
    "    # we set detail to 0 because we do not care about positioning data\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to fetch files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA not available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "CUDA not available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "#Change this path to your folder containing files to detect PII from. \n",
    "\n",
    "path = r\"/Users/nirav/Documents/SPU/DS-630 Machine Learning/ProjectInputFiles/\"\n",
    "  \n",
    "# Change the directory\n",
    "os.chdir(path)\n",
    "  \n",
    "# Read text File  \n",
    "\"\"\"\"  \n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        f.read()\n",
    "  \n",
    "  \"\"\"\n",
    "\n",
    "# iterate through all file\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        #file_path = f\"{path}/{file}\"\n",
    "        txtfiles = []\n",
    "        for file in glob.glob(\"*.txt\"):\n",
    "            txtfiles.append(file)\n",
    "\n",
    "        ff = open(path + txtfiles[0])\n",
    "        # call read text file function\n",
    "        txt_f = ff.read()    \n",
    "\n",
    "    elif file.endswith(\".csv\"):\n",
    "        csvfiles = []\n",
    "        for file2 in glob.glob(\"*.csv\"):\n",
    "            csvfiles.append(file2)\n",
    "\n",
    "        ff2 = pd.read_csv(path + csvfiles[0])\n",
    "\n",
    "        csv_f = str(ff2)\n",
    "\n",
    "    elif file.endswith(\".docx\"):\n",
    "        docfiles = []\n",
    "        for file4 in glob.glob(\"*.docx\"):\n",
    "            docfiles.append(file4)\n",
    "\n",
    "        ff4 = docx2txt.process(path + docfiles[0])\n",
    "\n",
    "        doc_f = ff4\n",
    "\n",
    "    elif file.endswith(\".pdf\"):\n",
    "        pdffiles = []\n",
    "        for file5 in glob.glob(\"*.pdf\"):\n",
    "            pdffiles.append(file5)\n",
    "\n",
    "        ff5 = textract.process(path + pdffiles[0], method='pdfminer')\n",
    "\n",
    "        pdf_f = str(ff5)\n",
    "\n",
    "\n",
    "    elif file.endswith(\".jpg\"):\n",
    "        jpgfiles = []\n",
    "        for file6 in glob.glob(\"*.jpg\"):\n",
    "            jpgfiles.append(file6)\n",
    "\n",
    "        ff6 = Image.open(path + jpgfiles[0])\n",
    "        jpg_f = ocr_pii(ff6)\n",
    "        #jpg_f = ff6\n",
    "\n",
    "\n",
    "    elif file.endswith(\".png\"):\n",
    "        pngfiles = []\n",
    "        for file7 in glob.glob(\"*.png\"):\n",
    "            pngfiles.append(file7)\n",
    "\n",
    "        ff7 = Image.open(path + pngfiles[0])\n",
    "        png_f = ocr_pii(ff7)\n",
    "        #png_f = ff7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumbersRecognizer(EntityRecognizer):\n",
    "    \n",
    "    expected_confidence_level = 0.7 # expected confidence level for this recognizer\n",
    "    \n",
    "    def load(self): # -> None\n",
    "        \"\"\"No loading is required.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def analyze(\n",
    "        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n",
    "    ): # -> List[RecognizerResult]\n",
    "        \"\"\"\n",
    "        Analyzes test to find tokens which represent numbers (either 123 or One Two Three).\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # iterate over the spaCy tokens, and call `token.like_num`\n",
    "        for token in nlp_artifacts.tokens:\n",
    "            if token.like_num:\n",
    "                result = RecognizerResult(\n",
    "                    entity_type=\"NUMBER\",\n",
    "                    start=token.idx,\n",
    "                    end=token.idx + len(token),\n",
    "                    score=self.expected_confidence_level\n",
    "                )\n",
    "                results.append(result)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name pii.txt contains PII data.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    analyzer = AnalyzerEngine()\n",
    "\n",
    "\n",
    "\n",
    "    titles_recognizer = PatternRecognizer(supported_entity=\"TITLE\",\n",
    "                                        deny_list=[\"Mr.\",\"Mrs.\",\"Miss\"])\n",
    "\n",
    "    new_numbers_recognizer = NumbersRecognizer(supported_entities=[\"NUMBER\"])\n",
    "\n",
    "    #text3 = \"Nirav lives in Five 10 Broad st.\"\n",
    "    analyzer.registry.add_recognizer(new_numbers_recognizer)\n",
    "\n",
    "    numbers_results_txt = analyzer.analyze(text=txt_f, language=\"en\")\n",
    "    #print(\"Results:\")\n",
    "    #print(\"\\n\".join([str(res) for res in numbers_results_txt]))\n",
    "\n",
    "    phone_recognizer = PatternRecognizer(supported_entity=\"PHONE_NUMBER\",\n",
    "                                        deny_list=\"1234567890\" or \"123-456-7890\")\n",
    "\n",
    "    analyzer.registry.add_recognizer(titles_recognizer)\n",
    "    analyzer.registry.add_recognizer(phone_recognizer)\n",
    "\n",
    "    analyzer_results_txt = analyzer.analyze(text=txt_f,\n",
    "                                entities=[\"TITLE\", \"PHONE_NUMBER\"],\n",
    "                                language='en')\n",
    "    #print(analyzer_results)\n",
    "\n",
    "    analyzer_dict_txt = analyzer_results_txt\n",
    "\n",
    "\n",
    "\n",
    "    if analyzer_results_txt and numbers_results_txt is not None:\n",
    "        print(\"File name \" + txtfiles[0] + \" contains PII data.\" )\n",
    "\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    \n",
    "    analyzer = AnalyzerEngine()\n",
    "    titles_recognizer = PatternRecognizer(supported_entity=\"TITLE\",\n",
    "                                        deny_list=[\"Mr.\",\"Mrs.\",\"Miss\"])\n",
    "\n",
    "    new_numbers_recognizer = NumbersRecognizer(supported_entities=[\"NUMBER\"])\n",
    "\n",
    "    #text3 = \"Nirav lives in Five 10 Broad st.\"\n",
    "    analyzer.registry.add_recognizer(new_numbers_recognizer)\n",
    "\n",
    "    numbers_results_csv = analyzer.analyze(text=csv_f, language=\"en\")\n",
    "    #print(\"Results:\")\n",
    "    #print(\"\\n\".join([str(res) for res in numbers_results_csv]))\n",
    "\n",
    "    phone_recognizer = PatternRecognizer(supported_entity=\"PHONE_NUMBER\",\n",
    "                                        deny_list=\"1234567890\" or \"123-456-7890\")\n",
    "\n",
    "    analyzer.registry.add_recognizer(titles_recognizer)\n",
    "    analyzer.registry.add_recognizer(phone_recognizer)\n",
    "\n",
    "    analyzer_results_csv = analyzer.analyze(text=csv_f,\n",
    "                                entities=[\"TITLE\", \"PHONE_NUMBER\"],\n",
    "                                language='en')\n",
    "    #print(analyzer_results)\n",
    "\n",
    "    if analyzer_results_csv and numbers_results_csv is not None:\n",
    "        print(\"File name \" + csvfiles[0] + \" contains PII data.\" )\n",
    "\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name pii.docx contains PII data.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    analyzer = AnalyzerEngine()\n",
    "    titles_recognizer = PatternRecognizer(supported_entity=\"TITLE\",\n",
    "                                        deny_list=[\"Mr.\",\"Mrs.\",\"Miss\"])\n",
    "\n",
    "    new_numbers_recognizer = NumbersRecognizer(supported_entities=[\"NUMBER\"])\n",
    "\n",
    "    #text3 = \"Nirav lives in Five 10 Broad st.\"\n",
    "    analyzer.registry.add_recognizer(new_numbers_recognizer)\n",
    "\n",
    "    numbers_results_doc = analyzer.analyze(text=doc_f, language=\"en\")\n",
    "    #print(\"Results:\")\n",
    "    #print(\"\\n\".join([str(res) for res in numbers_results_doc]))\n",
    "\n",
    "    phone_recognizer = PatternRecognizer(supported_entity=\"PHONE_NUMBER\",\n",
    "                                        deny_list=\"1234567890\" or \"123-456-7890\")\n",
    "\n",
    "    analyzer.registry.add_recognizer(titles_recognizer)\n",
    "    analyzer.registry.add_recognizer(phone_recognizer)\n",
    "\n",
    "    analyzer_results_doc = analyzer.analyze(text=doc_f,\n",
    "                                entities=[\"TITLE\", \"PHONE_NUMBER\"],\n",
    "                                language='en')\n",
    "    #print(analyzer_results)\n",
    "\n",
    "    if analyzer_results_doc and numbers_results_doc is not None:\n",
    "        print(\"File name \" + docfiles[0] + \" contains PII data.\" )\n",
    "\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    analyzer = AnalyzerEngine()\n",
    "    titles_recognizer = PatternRecognizer(supported_entity=\"TITLE\",\n",
    "                                        deny_list=[\"Mr.\",\"Mrs.\",\"Miss\"])\n",
    "\n",
    "    new_numbers_recognizer = NumbersRecognizer(supported_entities=[\"NUMBER\"])\n",
    "\n",
    "    #text3 = \"Nirav lives in Five 10 Broad st.\"\n",
    "    analyzer.registry.add_recognizer(new_numbers_recognizer)\n",
    "\n",
    "    numbers_results_pdf = analyzer.analyze(text=pdf_f, language=\"en\")\n",
    "    #print(\"Results:\")\n",
    "    #print(\"\\n\".join([str(res) for res in numbers_results_pdf]))\n",
    "\n",
    "    phone_recognizer = PatternRecognizer(supported_entity=\"PHONE_NUMBER\",\n",
    "                                        deny_list=\"1234567890\" or \"123-456-7890\")\n",
    "\n",
    "    analyzer.registry.add_recognizer(titles_recognizer)\n",
    "    analyzer.registry.add_recognizer(phone_recognizer)\n",
    "\n",
    "    analyzer_results_pdf = analyzer.analyze(text=pdf_f,\n",
    "                                entities=[\"TITLE\", \"PHONE_NUMBER\"],\n",
    "                                language='en')\n",
    "    #print(analyzer_results)\n",
    "\n",
    "    if analyzer_results_pdf and numbers_results_pdf is not None:\n",
    "        print(\"File name \" + pdffiles[0] + \" contains PII data.\" )\n",
    "\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name license.jpg contains PII data.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    analyzer = AnalyzerEngine()\n",
    "    titles_recognizer = PatternRecognizer(supported_entity=\"TITLE\",\n",
    "                                        deny_list=[\"Mr.\",\"Mrs.\",\"Miss\"])\n",
    "\n",
    "    new_numbers_recognizer = NumbersRecognizer(supported_entities=[\"NUMBER\"])\n",
    "\n",
    "    #text3 = \"Nirav lives in Five 10 Broad st.\"\n",
    "    analyzer.registry.add_recognizer(new_numbers_recognizer)\n",
    "\n",
    "    numbers_results_jpg = analyzer.analyze(text=str(jpg_f), language=\"en\")\n",
    "    #print(\"Results:\")\n",
    "    #print(\"\\n\".join([str(res) for res in numbers_results_pdf]))\n",
    "\n",
    "    phone_recognizer = PatternRecognizer(supported_entity=\"PHONE_NUMBER\",\n",
    "                                        deny_list=\"1234567890\" or \"123-456-7890\")\n",
    "\n",
    "    analyzer.registry.add_recognizer(titles_recognizer)\n",
    "    analyzer.registry.add_recognizer(phone_recognizer)\n",
    "\n",
    "    analyzer_results_jpg = analyzer.analyze(text=str(jpg_f),\n",
    "                                entities=[\"TITLE\", \"PHONE_NUMBER\"],\n",
    "                                language='en')\n",
    "    #print(analyzer_results)\n",
    "\n",
    "    if analyzer_results_jpg and numbers_results_jpg is not None:\n",
    "        print(\"File name \" + jpgfiles[0] + \" contains PII data.\" )\n",
    "\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name license copy.png contains PII data.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    analyzer = AnalyzerEngine()\n",
    "    titles_recognizer = PatternRecognizer(supported_entity=\"TITLE\",\n",
    "                                        deny_list=[\"Mr.\",\"Mrs.\",\"Miss\"])\n",
    "\n",
    "    new_numbers_recognizer = NumbersRecognizer(supported_entities=[\"NUMBER\"])\n",
    "\n",
    "    #text3 = \"Nirav lives in Five 10 Broad st.\"\n",
    "    analyzer.registry.add_recognizer(new_numbers_recognizer)\n",
    "\n",
    "    numbers_results_png = analyzer.analyze(text=str(png_f), language=\"en\")\n",
    "    #print(\"Results:\")\n",
    "    #print(\"\\n\".join([str(res) for res in numbers_results_pdf]))\n",
    "\n",
    "    phone_recognizer = PatternRecognizer(supported_entity=\"PHONE_NUMBER\",\n",
    "                                        deny_list=\"1234567890\" or \"123-456-7890\")\n",
    "\n",
    "    analyzer.registry.add_recognizer(titles_recognizer)\n",
    "    analyzer.registry.add_recognizer(phone_recognizer)\n",
    "\n",
    "    analyzer_results_png = analyzer.analyze(text=str(png_f),\n",
    "                                entities=[\"TITLE\", \"PHONE_NUMBER\"],\n",
    "                                language='en')\n",
    "    #print(analyzer_results)\n",
    "\n",
    "    if analyzer_results_png and numbers_results_png is not None:\n",
    "        print(\"File name \" + pngfiles[0] + \" contains PII data.\" )\n",
    "\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    path_for_txt = Path(path + txtfiles[0])\n",
    "    owner_for_txt = path_for_txt.owner()\n",
    "    filePath_for_txt = (path + txtfiles[0])\n",
    "    fileStatsObj_for_txt = os.stat ( filePath_for_txt )\n",
    "    modificationTime_for_txt = time.ctime ( fileStatsObj_for_txt [ stat.ST_MTIME ] )\n",
    "    #print(str(path_for_txt.name + ' is owned by ' + owner_for_txt + ' ') + str(modificationTime_for_txt) + str(numbers_results_txt))\n",
    "    txt_info = [str(path_for_txt.name) + ' is owned by ' + owner_for_txt + ', Last modified on: ' + str(modificationTime_for_txt)]\n",
    "\n",
    "except IndexError:\n",
    "    path_for_txt = ''\n",
    "    owner_for_txt = ''\n",
    "    filePath_for_txt = ''\n",
    "    fileStatsObj_for_txt = ''\n",
    "    modificationTime_for_txt = ''\n",
    "    txt_nofile = ['Section for TXT output - No TXT file found.']\n",
    "    numbers_results_txt = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    path_for_csv = Path(path + csvfiles[0])\n",
    "    owner_for_csv = path_for_csv.owner()\n",
    "    filePath_for_csv = (path + csvfiles[0])\n",
    "    fileStatsObj_for_csv = os.stat ( filePath_for_csv )\n",
    "    modificationTime_for_csv = time.ctime ( fileStatsObj_for_csv [ stat.ST_MTIME ] )\n",
    "    #print(str(path_for_csv.name + ' is owned by ' + owner_for_csv + ' ') + str(modificationTime_for_csv) + str(numbers_results_csv))\n",
    "    csv_info = [str(path_for_csv.name) + ' is owned by ' + owner_for_csv + ', Last modified on: ' + str(modificationTime_for_csv)]\n",
    "\n",
    "except NameError:\n",
    "    path_for_csv = ''\n",
    "    owner_for_csv = ''\n",
    "    filePath_for_csv = ''\n",
    "    modificationTime_for_csv = ''\n",
    "    fileStatsObj_for_csv = ''\n",
    "    csv_info = ['Section for CSV output - No CSV file found.']\n",
    "    numbers_results_csv = ['']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    path_for_docx = Path(path + docfiles[0])\n",
    "    owner_for_docx = path_for_docx.owner()\n",
    "    filePath_for_docx = (path + docfiles[0])\n",
    "    fileStatsObj_for_docx = os.stat ( filePath_for_docx )\n",
    "    modificationTime_for_docx = time.ctime ( fileStatsObj_for_docx [ stat.ST_MTIME ] )\n",
    "    #print(str(path_for_docx.name + ' is owned by ' + owner_for_docx + ' ') + str(modificationTime_for_docx) + str(numbers_results_docx))\n",
    "    doc_info = [str(path_for_docx.name) + ' is owned by ' + owner_for_docx + ', Last modified on: ' + str(modificationTime_for_docx)]\n",
    "\n",
    "except IndexError:\n",
    "    path_for_docx = ''\n",
    "    owner_for_docx = ''\n",
    "    filePath_for_docx = ''\n",
    "    modificationTime_for_docx = ''\n",
    "    fileStatsObj_for_docx = ''\n",
    "    doc_info = ['Section for DOC output - No DOC file found.']\n",
    "    numbers_results_doc = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    path_for_pdf = Path(path + pdffiles[0])\n",
    "    owner_for_pdf = path_for_pdf.owner()\n",
    "    filePath_for_pdf = (path + pdffiles[0])\n",
    "    fileStatsObj_for_pdf = os.stat ( filePath_for_pdf )\n",
    "    modificationTime_for_pdf = time.ctime ( fileStatsObj_for_pdf [ stat.ST_MTIME ] )\n",
    "    #print(str(path_for_pdf.name + ' is owned by ' + owner_for_pdf + ' ') + str(modificationTime_for_pdf) + str(numbers_results_pdf))\n",
    "    pdf_info = [str(path_for_pdf.name) + ' is owned by ' + owner_for_pdf + ', Last modified on: ' + str(modificationTime_for_pdf)]\n",
    "\n",
    "except NameError:\n",
    "    path_for_pdf = ''\n",
    "    owner_for_pdf = ''\n",
    "    filePath_for_pdf = ''\n",
    "    modificationTime_for_pdf = ''\n",
    "    fileStatsObj_for_pdf = ''\n",
    "    pdf_info = ['Section for PDF output - No PDF file found.']\n",
    "    numbers_results_pdf = ['']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    path_for_jpg = Path(path + jpgfiles[0])\n",
    "    owner_for_jpg = path_for_jpg.owner()\n",
    "    filePath_for_jpg = (path + jpgfiles[0])\n",
    "    fileStatsObj_for_jpg = os.stat ( filePath_for_jpg )\n",
    "    modificationTime_for_jpg = time.ctime ( fileStatsObj_for_jpg [ stat.ST_MTIME ] )\n",
    "    #print(str(path_for_pdf.name + ' is owned by ' + owner_for_pdf + ' ') + str(modificationTime_for_pdf) + str(numbers_results_pdf))\n",
    "    jpg_info = [str(path_for_jpg.name) + ' is owned by ' + owner_for_jpg + ', Last modified on: ' + str(modificationTime_for_jpg)]\n",
    "\n",
    "except IndexError:\n",
    "    path_for_jpg = ''\n",
    "    owner_for_jpg = ''\n",
    "    filePath_for_jpg = ''\n",
    "    modificationTime_for_jpg = ''\n",
    "    fileStatsObj_for_jpg = ''\n",
    "    jpg_info = ['Section for JPG output - No JPG file found.']\n",
    "    numbers_results_jpg = ['']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    path_for_png = Path(path + pngfiles[0])\n",
    "    owner_for_png = path_for_png.owner()\n",
    "    filePath_for_png = (path + pngfiles[0])\n",
    "    fileStatsObj_for_png = os.stat ( filePath_for_png )\n",
    "    modificationTime_for_png = time.ctime ( fileStatsObj_for_png [ stat.ST_MTIME ] )\n",
    "    #print(str(path_for_png.name + ' is owned by ' + owner_for_png + ' ') + str(modificationTime_for_png) + str(numbers_results_png))\n",
    "    png_info = [str(path_for_png.name) + ' is owned by ' + owner_for_png + ', Last modified on: ' + str(modificationTime_for_png)]\n",
    "\n",
    "except IndexError:\n",
    "    path_for_png = ''\n",
    "    owner_for_png = ''\n",
    "    filePath_for_png = ''\n",
    "    modificationTime_for_png = ''\n",
    "    fileStatsObj_for_png = ''\n",
    "    png_info = ['Section for PNG output - No PNG file found.']\n",
    "    numbers_results_png = ['']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_op = (csv_info) + (numbers_results_csv + [\"\",\"\"]) + (txt_info) + (numbers_results_txt + [\"\",\"\"]) + (pdf_info) + (numbers_results_pdf + [\"\",\"\"]) + (doc_info) + (numbers_results_doc + [\"\",\"\"]) + (jpg_info) + (numbers_results_jpg + [\"\",\"\"]) + (png_info) + (numbers_results_png + [\"\",\"\"])\n",
    "\n",
    "final_op\n",
    "\n",
    "df = pd.DataFrame(final_op)\n",
    "\n",
    "#Change the path and locate your ouput folder, this is where the CSV containing detected PII will export.\n",
    "\n",
    "df.to_csv('/Users/nirav/Documents/SPU/DS-630 Machine Learning/ProjectOutputFiles/pii_output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete = ['Output generated']\n",
    "\n",
    "df = pd.DataFrame(complete)\n",
    "\n",
    "df.to_csv('/Users/nirav/Documents/SPU/DS-630 Machine Learning/ProjectOutputFiles/Output_generated.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
